{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e53d10",
   "metadata": {},
   "source": [
    "# Evaluation of generative question answering results\n",
    "\n",
    "with an view toward syntactical differences in the prompt.\n",
    "\n",
    "Please note: any non-blog-demo use case would likely make use of some experiment tracking framework. I use DIY code and Jupyter to keep things self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a158e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import difflib\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pyarrow as pa\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b2fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_prompt_config(left_run_config, right_run_config) -> List:\n",
    "    d = difflib.Differ()\n",
    "    prompt_config_keys = ['context', 'question', 'answer_format', 'template']\n",
    "    diffs = {}\n",
    "    keys_with_diff = []\n",
    "    for key in prompt_config_keys:\n",
    "\n",
    "        diff = d.compare(\n",
    "            left_run_config[key], right_run_config[key]\n",
    "        )\n",
    "        # Remove match comparison results; see https://docs.python.org/3/library/difflib.html#difflib.Differ\n",
    "        diff = [diff_char for diff_char in diff if '  ' != diff_char[:2]]\n",
    "        diffs[key] = diff\n",
    "        if diff:\n",
    "            keys_with_diff.append(key)\n",
    "    \n",
    "    # Get summary of diffs\n",
    "    print(f'Keys with diff: {keys_with_diff}')\n",
    "    \n",
    "    for key, value in diffs.items():\n",
    "        print(key)\n",
    "        pprint(value)\n",
    "    return keys_with_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e190ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_table = pa.Table.from_pylist([\n",
    "    {\n",
    "        'Platz': 1,\n",
    "        'Schwimmer': 'Thomas Ehrhardt',\n",
    "        'JG': 1977,\n",
    "        'Verein': 'SSKC Poseidon Aschaffenburg',\n",
    "        'Zeit': '1:01,64',\n",
    "        'Punkte': 465,\n",
    "        'Ort': 'Gwangju',\n",
    "        'Datum': '8/2019'\n",
    "    },\n",
    "   {\n",
    "       'Platz': 2,\n",
    "        'Schwimmer': 'Jochen Kaminski',\n",
    "        'JG': 1974,\n",
    "        'Verein': 'SSF Bonn 05',\n",
    "        'Zeit': '1:03,91',\n",
    "        'Punkte': 417,\n",
    "        'Ort': 'Karlsruhe',\n",
    "        'Datum': '6/2019'\n",
    "   },\n",
    "   {\n",
    "       'Platz': 3,\n",
    "        'Schwimmer': 'Paul Larsen',\n",
    "        'JG': 1977,\n",
    "        'Verein': 'TSV Haar',\n",
    "        'Zeit': '1:05,01',\n",
    "        'Punkte': 397,\n",
    "        'Ort': 'Kranj',\n",
    "        'Datum': '9/2018'\n",
    "   },\n",
    "   {\n",
    "       'Platz': 4,\n",
    "        'Schwimmer': 'Sebastian Kratzenstein',\n",
    "        'JG': 1978,\n",
    "        'Verein': 'BSC Robben',\n",
    "        'Zeit': '1:05,21',\n",
    "        'Punkte': 393,\n",
    "        'Ort': 'Karlsruhe',\n",
    "        'Datum': '6/2019'\n",
    "   },\n",
    "   {\n",
    "       'Platz': 5,\n",
    "       'Schwimmer': 'Torben Kritzer',\n",
    "       'JG': 1977,\n",
    "       'Verein': 'Bad Homburger SC 1927',\n",
    "       'Zeit': '1:05,64',\n",
    "       'Punkte': 385,\n",
    "       'Ort': 'Karlsruhe',\n",
    "       'Datum': '6/2019'}\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2274a5",
   "metadata": {},
   "source": [
    "## Group together the experiment runs\n",
    "\n",
    "into runs that failed to produce the expected schema, ones that produced the expected schema but got the answer wrong in some other way, and runs that yielded a correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d03bf782",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path(os.environ['PROJECT_ROOT']) / 'generative-question-answering' / 'outputs' / 'blog-all-combos'\n",
    "\n",
    "run_dirs = [elt for elt in results_dir.iterdir() if elt.is_dir()]\n",
    "schema_successes = []\n",
    "schema_failures = []\n",
    "\n",
    "for run_dir in run_dirs:\n",
    "    with open(run_dir / 'result.json', 'r') as fp:\n",
    "        result = json.load(fp)\n",
    "    with open(run_dir / '.hydra' / 'config.yaml') as fp:\n",
    "        conf = OmegaConf.load(fp)\n",
    "        conf = OmegaConf.to_object(conf)\n",
    "    if result['data_contract_success']:\n",
    "        schema_successes.append({\n",
    "            'response': result['response'], 'config': conf, 'run_dir': run_dir.as_posix()})\n",
    "    else:\n",
    "        schema_failures.append({'response': result['response'], 'config': conf, 'run_dir': run_dir.as_posix()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d93cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_result = []\n",
    "expected_schema_wrong_result = []\n",
    "\n",
    "for run in schema_successes:\n",
    "    if pa.Table.from_pylist(run['response']).equals(expected_table):\n",
    "        expected_result.append(run)\n",
    "    else:\n",
    "        expected_schema_wrong_result.append(run)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a8ac24",
   "metadata": {},
   "source": [
    "## Look at successful runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93d5452e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generate_parent_module': 'openai', 'generate_object': 'ChatCompletion', 'generate_method': 'create', 'model_version': 'gpt-3.5-turbo-0301', 'model_params': {'temperature': 0}, 'response_parser_parent_module': 'mp_blog.llm_utils', 'response_parser_method': 'openai_chat_completion_parser', 'context': 'Quelle: https://www.dsv.de/schwimmen/wettkampf-national/bestenlisten/\\n\\nAuswahl\\nGeschlecht:\\nM\\tW\\tX\\nBahn:\\n25m\\t50m\\nStrecke:\\n\\n100m Schmetterling\\nZeitbereich:\\n\\nSaison 2018/2019\\nAltersklasse:\\n\\nAK 40 - JG 1974 - 1978\\nPunkte:\\n\\nFINA 2022 (25m)\\nRegion:\\n\\nDeutschland\\n\\n\\n25 Einträge\\nSuche\\nDeutscher Rekord: 0:51,19 von Steffen Deibler (Hamburger SC r.V. von 1879) am 28.04.2013\\nPlatz\\tSchwimmer\\tJG\\tVerein\\tZeit\\tPunkte\\tOrt\\tDatum\\n1\\tThomas Ehrhardt\\t1977\\tSSKC Poseidon Aschaffenburg\\t1:01,64\\t465\\tGwangju\\t8/2019\\n2\\tJochen Kaminski\\t1974\\tSSF Bonn 05\\t1:03,91\\t417\\tKarlsruhe\\t6/2019\\n3\\tPaul Larsen\\t1977\\tTSV Haar\\t1:05,01\\t397\\tKranj\\t9/2018\\n4\\tSebastian Kratzenstein\\t1978\\tBSC Robben\\t1:05,21\\t393\\tKarlsruhe\\t6/2019\\n5\\tTorben Kritzer\\t1977\\tBad Homburger SC 1927\\t1:05,64\\t385\\tKarlsruhe\\t6/2019\\n6\\tRalf Hildebrandt\\t1978\\tDresdner SC 1898\\t1:06,13\\t377\\tRiesa\\t5/2019\\n7\\tMatthias Michaelsen\\t1977\\tSG Böhmetal\\t1:06,47\\t371\\tBraunschweig\\t3/2019\\n8\\tLars Kellermann\\t1974\\tSG Badenweiler-Neuenburg\\t1:07,07\\t361\\tKarlsruhe\\t6/2019\\n9\\tChristian Hylla\\t1978\\tSSF Bonn 05\\t1:07,60\\t353\\tKamen\\t6/2019\\n10\\tKieran Garbutt\\t1976\\tSV Bayreuth\\t1:08,48\\t339\\tPappenheim\\t7/2019\\n\\n\\nQuelle: https://www.dsv.de/schwimmen/wettkampf-national/bestenlisten/\\n\\nAuswahl\\nGeschlecht:\\nM\\tW\\tX\\nBahn:\\n25m\\t50m\\nStrecke:\\n\\n100m Schmetterling\\nZeitbereich:\\n\\nSaison 2019/2020\\nAltersklasse:\\n\\nAK 40 - JG 1975 - 1979\\nPunkte:\\n\\nFINA 2022 (25m)\\nRegion:\\n\\nDeutschland\\n\\n\\n2 Einträge\\nSuche\\nDeutscher Rekord: 0:51,19 von Steffen Deibler (Hamburger SC r.V. von 1879) am 28.04.2013\\nPlatz\\tSchwimmer\\tJG\\tVerein\\tZeit\\tPunkte\\tOrt\\tDatum\\n1\\tRalf Hildebrandt\\t1978\\tDresdner SC 1898\\t1:05,66\\t385\\tDresden\\t12/2019\\n2\\tIvo Rudolph\\t1978\\tSV Grün-Weiß Wittenberg\\t1:12,48\\t286\\tDresden\\t12/2019\\n', 'question': 'Was waren die top 5 Zeiten Deutschlands der Saison zwischen 2018 und 2020 der Männer auf 100m Schmetterling im Altersklasse 40-44?', 'answer_format': 'Return only a json document as a list entries with keys `Platz, Schwimmer, JG, Verein, Zeit, Punkte, Ort, Datum`.', 'template': 'Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer format: {answer_format}\\n\\nAnswer:\\n'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for run in expected_result:\n",
    "    print(run['config'])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5596d3b5",
   "metadata": {},
   "source": [
    "### and the diffs between the two successful runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "333d859f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m diff_prompt_config(expected_result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mexpected_result\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "diff_prompt_config(expected_result[0]['config'], expected_result[1]['config'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f29b649",
   "metadata": {},
   "source": [
    "which tells us that `ae` vs `ä` in the word 'Männer' makes no difference for these runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9448df15",
   "metadata": {},
   "source": [
    "## Analyze runs with the correct schema but wrong result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da4b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_schema_wrong_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c62a9e",
   "metadata": {},
   "source": [
    "## and finally look at the failed runs\n",
    "\n",
    "Let's filter out the runs with blank context, as they yield the usual \"Sorry, as an AI language model, I don't have access to the specific information you requested\" or \"I'm sorry, I cannot provide an answer to this question as I am an AI language model and do not have access to current or historical sports data\" responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00205f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "failures_sans_context = [run for run in schema_failures if not run['config']['context']]\n",
    "for run in failures_sans_context[:3]:\n",
    "    print(run['response'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d690de0c",
   "metadata": {},
   "source": [
    "Let's see if there is any clue to the unexpected json schema by diffing with the first of the correct results. Below we'll drill down more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bc5da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "failures_w_context = [run for run in schema_failures if run['config']['context']]\n",
    "\n",
    "diff_keys_per_run = {}\n",
    "for idx, run in enumerate(failures_w_context):\n",
    "    print(f'Failure with non-empty context run {idx}')\n",
    "    keys_with_diff = diff_prompt_config(expected_result[0]['config'], run['config'])\n",
    "    diff_keys_per_run[idx] = keys_with_diff\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a9048c",
   "metadata": {},
   "source": [
    "## Failure diffs in detail\n",
    "\n",
    "Let's first look at the failed runs for which only one of `question`, `answer_format` or `template` was different from the first successful run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35da5cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_keys_per_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d3460",
   "metadata": {},
   "source": [
    "### question diff failures\n",
    "\n",
    "The differences to the correct response were that the question was posed in English in run idx `0` and with a `ae` rather than `ä` in the word \"Männer\" in run index `5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d0883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "print(f\"Incorrect response: {failures_w_context[idx]['response']}\\n\")\n",
    "print(f\"question: {failures_w_context[idx]['config']['question']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b1e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "\n",
    "print(f\"Incorrect response: {failures_w_context[idx]['response']}\\n\")\n",
    "print(f\"question: {failures_w_context[idx]['config']['question']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4335a415",
   "metadata": {},
   "source": [
    "At first glance, these responses looks correct. The reason it failed our data contract and was hence deemed incorrect is due to the types of \"JG\" (Jahrgang, i.e. year of birth) and \"Punkte\" (i.e. points) when the question is phrased in English.\n",
    "\n",
    "That's at least one explanation, but it feels a little far-fetched. It could just be the inherent randomness of generative AI--the same input will yield different outputs with some non-zero probability (unless you force probabilities to zero as with e.g. [context-free-grammars](https://matt-rickard.com/context-free-grammar-parsing-with-llms)).\n",
    "\n",
    "### Question in English: Maybe it's just LLM randomness?\n",
    "\n",
    "To test the role of mixing an English question with German context in response data type errors, we ran the same prompt 10 times, putting results in [munichpavel.github.io/generative-question-answering/outputs/blog-english-question-repeats](https://munichpavel.github.io/generative-question-answering/outputs/blog-english-question-repeats).\n",
    "\n",
    "Let's see if there are any that get the correct answer thanks to the randomness of generative AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c59db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_english = Path(os.environ['PROJECT_ROOT']) / 'generative-question-answering' / 'outputs' / 'blog-english-question-repeats'\n",
    "schema_successes = []\n",
    "schema_failures = []\n",
    "\n",
    "run_dirs = [elt for elt in results_english.iterdir() if elt.is_dir()]\n",
    "\n",
    "for run_dir in run_dirs:\n",
    "    with open(run_dir / 'result.json', 'r') as fp:\n",
    "        result = json.load(fp)\n",
    "    with open(run_dir / '.hydra' / 'config.yaml') as fp:\n",
    "        conf = OmegaConf.load(fp)\n",
    "        conf = OmegaConf.to_object(conf)\n",
    "    if result['data_contract_success']:\n",
    "        schema_successes.append({\n",
    "            'response': result['response'], 'config': conf, 'run_dir': run_dir.as_posix()})\n",
    "    else:\n",
    "        schema_failures.append({'response': result['response'], 'config': conf, 'run_dir': run_dir.as_posix()})\n",
    "        \n",
    "print(f'Number of runs with schema successes {len(schema_successes)}')\n",
    "print(f'Number of runs with schema failures {len(schema_failures)}')\n",
    "\n",
    "print(f\"Response from run 10: {result['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72eaae6",
   "metadata": {},
   "source": [
    "We have not proved that giving German text as context with a question posed in English leads to worse results ceteris paribus, but if you are builing a LLM-service for this use case, it seems you should phrase your question in the same language as the context. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1e49dd",
   "metadata": {},
   "source": [
    "### Failure 1--answer format does note specify json-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "print(f\"Incorrect response: {failures_w_context[idx]['response']}\\n\")\n",
    "print(f\"answer_format: {failures_w_context[idx]['config']['answer_format']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8fd4f5",
   "metadata": {},
   "source": [
    "This failure is the most intuitive one. If you don't lock down the json schema enough, the LLM will guess for you. Anyone software developer would do this same given ambiguity (and an expert one would flag the ambiguity with the business counterpart before going too far)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22395ee8",
   "metadata": {},
   "source": [
    "### Failure 4 diffs--template with extra space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11504f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "\n",
    "print(f\"Incorrect response: {failures_w_context[idx]['response']}\\n\")\n",
    "\n",
    "print(f\"template: {failures_w_context[idx]['config']['template']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf23f2a7",
   "metadata": {},
   "source": [
    "So just adding an extra space before the context text begins is enough to yield a wrong answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d772c09",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "\n",
    "Why exactly the question-answering fails when\n",
    "\n",
    "* there is an extra space before the context section of the prompt template\n",
    "* the question is in English while the context is in German\n",
    "* the question uses `ae` rather than `ä` in the word \"Männer\"\n",
    "\n",
    "is beyond me. \n",
    "\n",
    "And that's likely OK, as the appeal of LLMs is that you can offload much of what your use-case needs to the LLM without understanding in full detail why or how it gives the results it does.\n",
    "\n",
    "What these examples show, however, is that it pays to manage the natural-language-syntax-as-configuration well, which in this post we've show by taking a standard approach from ML--experiment and configuration management.\n",
    "\n",
    "By using sound configuration management practices, we can get the upside of LLMs while managing the downsides, which in this use case include\n",
    "\n",
    "1. An extra space in the prompt template leading to wrong results.\n",
    "1. Asking an English question of German context leading to wrong data types in the response."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
